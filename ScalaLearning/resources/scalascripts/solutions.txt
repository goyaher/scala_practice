scala> sc.textFile("spark/wikipedia.dat")
scala> val filecontent = sc.textFile("spark/wikipedia.dat")
scala> filecontent.count()
res7: Long = 4086                                                               

scala> filecontent.flatMap(l => l.split(" "))
res8: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at flatMap at <console>:26

scala> val words = filecontent.flatMap(l => l.split(" "))
words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[5] at flatMap at <console>:25

scala> val wordCount = words.map(w => (w, 1)).reduceByKey(_+_)
wordCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[7] at reduceByKey at <console>:25

scala> wordCount.saveAsTextFile("spark/wikipedia_wordcount")
                                                                                
scala> val javaWordCount = wordCount.filter(t => (t._1.toUpperCase ==  "JAVA"))
javaWordCount: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[9] at filter at <console>:25

scala> javaWordCount.count()
res10: Long = 4                                                                 

scala> javaWordCount.foreach(println)
[Stage 6:>                                                          (0 + 2) / 2]
(JAVA,11)
(JAva,1)
(Java,2553)
(java,117)
                                                                                
scala> javaWordCount.saveAsTextFile("spark/wikipedia_java_count")

scala> val scalaFirst10Lines = filecontent.filter(l => l.contains("scala")).take(10)

scala> sc.parallelize(scalaFirst10Lines).saveAsTextFile("spark/wikipedia_scala10lines")